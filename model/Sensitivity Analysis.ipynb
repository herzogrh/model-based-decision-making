{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the problem and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frac1\\Anaconda3\\lib\\site-packages\\ema_workbench\\em_framework\\evaluators.py:22: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  'ipyparallel not installed - IpyparalleEvaluator not available')\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from ema_workbench import (Model, MultiprocessingEvaluator, perform_experiments, ema_logging, Policy, Scenario)\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from ema_workbench.analysis.scenario_discovery_util import RuleInductionType\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from ema_workbench.em_framework.samplers import sample_uncertainties\n",
    "from ema_workbench.util import ema_logging, save_results, load_results\n",
    "\n",
    "\n",
    "from SALib.analyze import sobol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification of the problem\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(1)\n",
    "\n",
    "# Specification of the policies\n",
    "\n",
    "policy_noaction = Policy(\"NoAction\",  \n",
    "                    **{'0_RfR 0': 0, '0_RfR 1' : 0, '0_RfR 2' : 0,\n",
    "                    '1_RfR 0': 0, '1_RfR 1' : 0, '1_RfR 2' : 0,\n",
    "                    '2_RfR 0': 0, '2_RfR 1' : 0, '2_RfR 2' : 0,\n",
    "                    '3_RfR 0': 0, '3_RfR 1' : 0, '3_RfR 2' : 0,\n",
    "                    '4_RfR 0': 0, '4_RfR 1' : 0, '4_RfR 2' : 0,\n",
    "                    'A.1_DikeIncrease 0' : 0, 'A.1_DikeIncrease 1' : 0, 'A.1_DikeIncrease 2' : 0,\n",
    "                    'A.2_DikeIncrease 0' : 0, 'A.2_DikeIncrease 1' : 0, 'A.2_DikeIncrease 2' : 0,\n",
    "                    'A.3_DikeIncrease 0' : 0, 'A.3_DikeIncrease 1' : 0, 'A.3_DikeIncrease 2' : 0,\n",
    "                    'A.4_DikeIncrease 0' : 0, 'A.4_DikeIncrease 1' : 0, 'A.4_DikeIncrease 2' : 0,\n",
    "                    'A.5_DikeIncrease 0' : 0, 'A.5_DikeIncrease 1' : 0, 'A.5_DikeIncrease 2' : 0,\n",
    "                    'EWS_DaysToThreat':  0   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sobol's scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Execute Sensistivity Analysis on uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a total of N(2K+2) = 10(2\\*19+2) experiments, since the dike model has K=19 uncertainties and we use a baseline number of experiments equalling N=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dike_model.uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the problem into an SA-equivalent problem\n",
    "\n",
    "sa_problem = get_SALib_problem(dike_model.uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 400 scenarios * 1 policies * 1 model(s) = 400 experiments\n",
      "[MainProcess/INFO] 40 cases completed\n",
      "[MainProcess/INFO] 80 cases completed\n",
      "[MainProcess/INFO] 120 cases completed\n",
      "[MainProcess/INFO] 160 cases completed\n",
      "[MainProcess/INFO] 200 cases completed\n",
      "[MainProcess/INFO] 240 cases completed\n",
      "[MainProcess/INFO] 280 cases completed\n",
      "[MainProcess/INFO] 320 cases completed\n",
      "[MainProcess/INFO] 360 cases completed\n",
      "[MainProcess/INFO] 400 cases completed\n",
      "[MainProcess/INFO] experiments finished\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sobol time is 5 mintues\n"
     ]
    }
   ],
   "source": [
    "# Execute Sensistivity Analysis over uncertainties\n",
    "\n",
    "baseline_n_experiments = 10\n",
    "start = time.time()\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "     experiments, outcomes = evaluator.perform_experiments(scenarios = baseline_n_experiments,\n",
    "                                                           policies = [policy_noaction], \n",
    "                                                           uncertainty_sampling = SOBOL)\n",
    "end = time.time()\n",
    "print('Sobol time is ' + str(round((end - start)/60)) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] results saved successfully to C:\\Users\\frac1\\Documents\\GitHub\\Model Based DM - Assignment collaboration\\model-based-decision-making\\model\\Sobol_results_uncertainties_200sc_NoActionpol.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# save the results\n",
    "\n",
    "u_results = (experiments, outcomes)\n",
    "save_results(u_results, 'Sobol_results_uncertainties_10ex_NoActionpol.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Execute Sensistivity Analysis on levers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a sensitivity analysis over a total of N(2K+2) = 10(2\\*31+2) experiments, since the dike model has K=31 levers and we use a baseline number of experiments equalling N=10 under the reference scenario (set as the worst-case scenario, and found from the notebook 1 - Reference Case Scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dike_model.levers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the problem into an SA-equivalent problem\n",
    "\n",
    "sa_problem = get_SALib_problem(dike_model.levers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] results loaded succesfully from C:\\Users\\frac1\\Documents\\GitHub\\Model Based DM - Assignment collaboration\\model-based-decision-making\\results\\500Scenarios_NoAction_PF1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Load policy results of a NoAction policy under 500 scenarios (notebook 1-Reference Case Scenario)\n",
    "[experiments, outcomes] = load_results(\"../results/500Scenarios_NoAction_PF1.tar.gz\")\n",
    "outcomes_df = pd.DataFrame(data=outcomes)\n",
    "\n",
    "#Get index of worst-case scenario\n",
    "index_wc = outcomes_df.sort_values(\"Expected Number of Deaths\").tail(1).index\n",
    "experiment_wc = experiments.iloc[index_wc]\n",
    "\n",
    "#Set the reference scenario as the worst-case scenario\n",
    "reference_scenarios = [Scenario(f\"{index}\", **row) for index, row in experiment_wc.iloc[0:,0:19].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 1 scenarios * 640 policies * 1 model(s) = 640 experiments\n",
      "[MainProcess/INFO] 64 cases completed\n",
      "[MainProcess/INFO] 128 cases completed\n",
      "[MainProcess/INFO] 192 cases completed\n",
      "[MainProcess/INFO] 256 cases completed\n",
      "[MainProcess/INFO] 320 cases completed\n",
      "[MainProcess/INFO] 384 cases completed\n",
      "[MainProcess/INFO] 448 cases completed\n",
      "[MainProcess/INFO] 512 cases completed\n",
      "[MainProcess/INFO] 576 cases completed\n",
      "[MainProcess/INFO] 640 cases completed\n",
      "[MainProcess/INFO] experiments finished\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sobol time is 8 mintues\n"
     ]
    }
   ],
   "source": [
    "# Execute Sensistivity Analysis over levers\n",
    "\n",
    "baseline_n_experiments = 10\n",
    "\n",
    "start = time.time()\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "     experiments, outcomes = evaluator.perform_experiments(scenarios = reference_scenarios,\n",
    "                                                           policies = baseline_n_experiments,\n",
    "                                                           levers_sampling = SOBOL)\n",
    "end = time.time()\n",
    "print('Sobol time is ' + str(round((end - start)/60)) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] results saved successfully to C:\\Users\\frac1\\Documents\\GitHub\\Model Based DM - Assignment collaboration\\model-based-decision-making\\model\\Sobol_results_levers_10ex_RefScenario.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#save the results\n",
    "\n",
    "l_results = (experiments, outcomes)\n",
    "save_results(l_results, 'Sobol_results_levers_10ex_RefScenario.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
